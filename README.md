# Parametric Curve — Parameter Estimation (Work Summary)

Short, structured documentation for estimating three unknown parameters (θ, M, X) of a parametric curve from measured (x, y) points for t in (6, 60).

---

Table of contents
- [Project snapshot](#project-snapshot)
- [Problem statement](#problem-statement)
- [Model](#model)
- [Unknowns / bounds](#unknowns--bounds)
- [Data](#data)
- [What I implemented](#what-i-implemented)
- [Methods & implementation notes](#methods--implementation-notes)
- [Metrics & diagnostics](#metrics--diagnostics)
- [How to reproduce (quick start)](#how-to-reproduce-quick-start)
- [Expected outputs](#expected-outputs)
- [Files in this repository (layout)](#files-in-this-repository-layout)
- [Literature & references](#literature--references)
- [License & contact](#license--contact)

---

Project snapshot
- Repository: https://github.com/Shashankkusu/Research-and-Development
- Description: Flame assessment (parametric curve parameter estimation)
- Language: Python

Problem statement
- Given a set of measured 2D points (x_i, y_i) generated by a parametric model for t in (6, 60), estimate the model parameters:
  - θ (theta), M, and X.
- Measurements may or may not include the corresponding t values. Two estimation workflows are provided:
  - Known t_i: standard nonlinear least-squares.
  - Unknown t_i: projection / joint-optimization strategies (bundle-adjustment style).

Model
- x(t) = t + cos(θ) - exp(t * M) * sin(0.3 * t) * sin(θ) + X  
- y(t) = 42 + t + sin(θ) + exp(t * M) * sin(0.3 * t) * cos(θ)  
- θ is used in radians in the implementation.

Unknowns and bounds
- θ: 0° < θ < 50°  → (0, 50 * π / 180) radians
- M: -0.05 < M < 0.05
- X: 0 < X < 100

Data
- Default data file: data/xy_data.csv
- The CSV may contain:
  - (t, x, y) columns; or
  - only (x, y) columns.
- The code detects the presence of t and selects the appropriate estimation strategy.

What I implemented (concise)
- Data ingestion and validation.
- Forward model functions for x(t; θ, M, X) and y(t; θ, M, X).
- Parameter-estimation routines:
  - Local nonlinear least-squares (scipy.optimize.least_squares) for known t.
  - Projection-based mapping (dense t grid + nearest assignment) for unknown t.
  - Joint optimization that includes unknown t_i in the parameter vector (bundle-adjustment style).
  - Optional global search via differential_evolution to seed local solvers.
- Evaluation plotting and result export (JSON + PNGs).

Methods & implementation notes
- Parameter conversions: θ input/output exposed in degrees (θ_deg) and stored in radians internally (theta_rad).
- Optimization:
  - Local: scipy.optimize.least_squares (supports bounds and Levenberg–Marquardt or trust-region reflective methods).
  - Global: scipy.optimize.differential_evolution to reduce risk of local minima; followed by local refinement.
- When t_i are unknown:
  - Projection approach is faster and often sufficient: sample t on dense grid, compute model points, assign measured points to nearest model samples, then optimize parameters.
  - Joint optimization treats each t_i as a variable with bounds [6, 60]; more accurate but increases dimensionality and runtime.
- Numerical note: exp(t * M) is bounded by design (for M bounds used, exp(60*0.05) ≈ 20); monitor if bounds change.

Metrics & diagnostics
- Per-point residual: r_i = sqrt((x_i - x_model(t_i))^2 + (y_i - y_model(t_i))^2)
- Reported metrics:
  - MAE (mean absolute error) of residuals
  - RMSE (root mean squared error)
  - L1 sampling-based curve distance (distance between uniformly-sampled predicted and expected curves)
- Visual diagnostics:
  - Fitted curve vs measured points
  - Residuals vs t (or index when t unknown)
  - Residual histogram
- Outputs are saved under results/ (JSON + plots).

How to reproduce (quick start)
1. Clone
   ```bash
   git clone https://github.com/Shashankkusu/Research-and-Development.git
   cd Research-and-Development
   ```

2. Setup environment
   ```bash
   python -m venv .venv
   # macOS / Linux
   source .venv/bin/activate
   # Windows (PowerShell)
   .\.venv\Scripts\Activate.ps1
   ```

3. Install dependencies
   ```bash
   pip install -r requirements.txt
   # or, directly:
   pip install numpy scipy pandas matplotlib seaborn jupyter
   ```

4. Run the estimation script
   ```bash
   python scripts/estimate_params.py --data data/xy_data.csv --out results/estimates.json
   ```
   Script options (examples)
   - --method {local,global}        Choose optimization strategy (default: local)
   - --n-samples N                  Number of t samples for projection-based approaches
   - --seed S                       Random seed for reproducibility

5. Inspect results
   - results/estimates.json contains final estimates and metrics
   - results/plots/ contains PNG plots (e.g., fitted_curve.png, residuals.png)
   - The notebook notebooks/param_estimation.ipynb contains exploratory analyses and visual diagnostics.

Expected outputs (examples)
- results/estimates.json
  ```json
  {
    "theta_rad": 0.5235987755982988,
    "theta_deg": 30,
    "M": 0.002,
    "X": 12.5,
    "mae": 0.12,
    "rmse": 0.21,
    "L1_metric": 0.18,
    "optimizer_info": { "method": "least_squares", "nfev": 123, "status": 1 }
  }
  ```
- results/plots/fitted_curve.png
- results/plots/residuals.png

Files in this repository (work layout)
- README.md — this structured project summary
- data/xy_data.csv — measured points (raw)
- notebooks/param_estimation.ipynb — exploratory notebook with code, plots, diagnostics
- scripts/estimate_params.py — command-line estimation script
- results/estimates.json — final parameter estimates and summary metrics (output)
- results/plots/ — saved figures (PNG)
- LICENSE — MIT license

Literature & references (concise)
- SciPy: scipy.optimize.least_squares — library docs; used for local nonlinear least-squares.
- Storn, R., & Price, K. (1997). Differential Evolution — classic DE algorithm; used for global search.
- Nocedal, J., & Wright, S. (2006). Numerical Optimization. Springer — reference for optimization strategies.
- Triggs, B., McLauchlan, P. F., Hartley, R. I., & Fitzgibbon, A. W. (2000). Bundle Adjustment — joint optimization of parameters and latent variables.
- Levenberg–Marquardt (online references) — practical notes for LM algorithm and curve-fitting.

Notes / scope
- This repository is an implementation and documentation of parameter-estimation work for a project assignment; it is not a formal research paper.
- The literature survey is lightweight and intended to indicate algorithmic provenance and practical references.

License
- MIT License.[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


Contact
- Repository owner: @Shashankkusu
- For questions or issues: open an issue in this repository.
